import os
import json
from openai import OpenAI
from tqdm import tqdm
from collections import defaultdict
import time
import sys
import re

# -------------------------- é…ç½®å‚æ•° --------------------------
# è¯·å°†æ­¤è·¯å¾„ä¿®æ”¹ä¸ºæ‚¨å®é™…çš„æ–‡ä»¶è·¯å¾„
# FILE_PATH = "/root/nas/ç‘é‡‘/EMR-Paper-data/generate_model/1103Uncleaned/yyjy.jsonl"
FILE_PATH = "./backfill_cleaned.jsonl"
OUTPUT_FILE_PATH = "./llama_calculation_results_backfill_cleaned.jsonl" # è¾“å‡ºæ–‡ä»¶åä¸ºå‰10æ¡
STATS_OUTPUT_PATH = "./llama_overall_dept_stats_backfill_cleaned.json"  # æ–°å¢ï¼šæ•´ä½“å’Œç§‘å®¤ç»Ÿè®¡æ–‡ä»¶
MODEL_NAME = " "
MAX_RETRIES = 3
LLM_TEMPERATURE = 0.01 # ä½¿ç”¨è¾ƒä½çš„æ¸©åº¦ä»¥è·å¾—ç¨³å®šçš„äº‹å®æ¯”å¯¹ç»“æœ
MAX_SAMPLES = 454 # é™åˆ¶å¤„ç†çš„æ ·æœ¬æ•°é‡

# -------------------------- API é…ç½® --------------------------
try:
    # âš ï¸ ç¡®ä¿ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY å·²è®¾ç½®ï¼Œæˆ–åœ¨æ­¤å¤„ç¡¬ç¼–ç ã€‚
    client = OpenAI(
        api_key=os.getenv("DASHSCOPE_API_KEY", " "),
        base_url=" "
    )
    print("âœ… API å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")
except Exception as e:
    print(f"âŒ API å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    sys.exit(1)

# -------------------------- æ ¸å¿ƒé€»è¾‘å‡½æ•° --------------------------
def safe_int_convert(value, default=0):
    """å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºintï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›é»˜è®¤å€¼"""
    if isinstance(value, int):
        return value
    elif isinstance(value, (str, float)):
        try:
            return int(float(value))
        except (ValueError, TypeError):
            pass
    elif isinstance(value, list) or isinstance(value, dict):
        # å¦‚æœæ˜¯æ„å¤–çš„listæˆ–dictï¼Œè®°å½•è­¦å‘Šå¹¶è¿”å›é»˜è®¤
        tqdm.write(f"âš ï¸ æ£€æµ‹åˆ°éæ•°å­—ç±»å‹ ({type(value)}): {value}ï¼Œè½¬æ¢ä¸ºé»˜è®¤å€¼ {default}")
        return default
    return default

def calculate_metrics(N, M, X, Y):
    """æ ¹æ®äº‹å®æ•°é‡è®¡ç®—ä¸‰ä¸ªæŒ‡æ ‡"""
    if N == 0:
        cfcr = 0.0
    else:
        cfcr = X / N # æ ¸å¿ƒäº‹å®è¦†ç›–ç‡
    if M == 0:
        fcr = 0.0
        fhr = 0.0
    else:
        fcr = (X + Y) / M # äº‹å®ç¬¦åˆç‡
        fhr = (M - X - Y) / M # äº‹å®å¹»è§‰ç‡
  
    return cfcr, fcr, fhr

def process_single_item(item, client):
    """å¤„ç†å•ä¸ªæ•°æ®é¡¹ï¼Œæ„å»ºPromptï¼Œè°ƒç”¨LLMå¹¶è®¡ç®—æŒ‡æ ‡"""
    gold_truth = item.get("output", "")
    model_summary = item.get("new_summary", "")
  
    # ä» instruction ä¸­æå–æ’°å†™å»ºè®®
    instruction_full = item.get("instruction", "")
    try:
        # å‡è®¾æ’°å†™å»ºè®®åœ¨ 'æ’°å†™å»ºè®®:' å
        instruction_advice = instruction_full.split("æ’°å†™å»ºè®®:")[1].strip()
    except IndexError:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†éš”ç¬¦ï¼Œåˆ™ä½¿ç”¨æ•´ä¸ª instruction ä½œä¸ºå»ºè®®ï¼Œå¹¶è¿›è¡Œæ¸…ç†
        instruction_advice = "ï¼›".join([line.strip() for line in instruction_full.split('\n') if line.strip()])
        if not instruction_advice:
            instruction_advice = "æ— æ˜ç¡®æ’°å†™å»ºè®®"
    # æ„å»º LLM Prompt ç»“æ„
    prompt_data = {
      "gold_truth": gold_truth,
      "model_summary": model_summary,
      "instruction_advice": instruction_advice
    }
  
    # æ„é€  LLM æç¤ºè¯ (Prompt)
    prompt = f"""
    è¯·ä¸¥æ ¼æ‰§è¡Œä»¥ä¸‹åŒ»ç–—æ–‡æœ¬äº‹å®æ¯”å¯¹å’Œåˆ†ç±»ä»»åŠ¡ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚è¿™æ˜¯è¾“å…¥æ•°æ®ï¼š
    {json.dumps(prompt_data, ensure_ascii=False, indent=2)}
    è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š
    1. æ‹†è§£ï¼šå°† 'gold_truth' æ‹†è§£æˆç¦»æ•£çš„ N_facts åˆ—è¡¨ã€‚å°† 'model_summary' æ‹†è§£æˆ M_facts åˆ—è¡¨ã€‚å°† 'instruction_advice' æ‹†è§£æˆ I_facts åˆ—è¡¨ã€‚
    2. æ¯”å¯¹ä¸åˆ†ç±» (Mä¸­çš„æ¯ä¸ªäº‹å®å¿…é¡»åˆ†ç±»)ï¼š
       - **æ ¸å¿ƒåŒ¹é… (X):** å¯¹äº M_facts ä¸­çš„äº‹å® m_iï¼Œé¦–å…ˆåˆ¤æ–­æ˜¯å¦ä¸ N_facts ä¸­æŸä¸ªäº‹å®**è¯­ä¹‰é«˜åº¦ä¸€è‡´**ã€‚
       - **æŒ‡ä»¤åŒ¹é… (Y):** å¦‚æœ m_i ä¸å±äº X ç±»ï¼Œåˆ™åˆ¤æ–­ m_i æ˜¯å¦åˆç†åœ°**æ»¡è¶³**äº† I_facts ä¸­çš„æŸä¸ªæŒ‡ä»¤è¦æ±‚ã€‚
       - **å¹»è§‰äº‹å® (H):** å¦‚æœ m_i æ—¢ä¸å±äº X ä¹Ÿä¸å±äº Yï¼Œåˆ™å½’ç±»ä¸º "H"ã€‚
    3. è¿”å›ï¼šè¿”å›ä¸€ä¸ªå•ä¸€çš„JSONå¯¹è±¡ï¼ŒåŒ…å« 'N_total', 'M_total', 'X_matched', 'Y_matched', 'N_facts_list', 'M_facts_list', 'I_facts_list' å’Œ 'match_details' åˆ—è¡¨ã€‚
       - 'N_total' å’Œ 'M_total' å¿…é¡»æ˜¯æ•´æ•°ï¼ˆåˆ—è¡¨é•¿åº¦ï¼‰ã€‚
       - 'match_details' æ˜¯æ¯ä¸ª M_facts çš„åˆ†ç±»åˆ—è¡¨ï¼Œæ¯ä¸ªé¡¹æ˜¯ dict å¦‚ {{'model_fact': 'äº‹å®æ–‡æœ¬', 'gold_fact': 'åŒ¹é…é‡‘æ ‡å‡†ï¼ˆå¦‚æœXï¼‰æˆ–ç©º', 'instruction_fact': 'åŒ¹é…æŒ‡ä»¤ï¼ˆå¦‚æœYï¼‰æˆ–ç©º', 'type': 'X' æˆ– 'Y' æˆ– 'H'}}ã€‚
    ä¸¥æ ¼è¿”å› JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡å­—æˆ–è§£é‡Šã€‚
    """
    # LLM API è°ƒç”¨ (å¸¦é‡è¯•æœºåˆ¶)
    llm_output = None
    for attempt in range(MAX_RETRIES):
        try:
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„äº‹å®é¡¹æ¯”å¯¹åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·è¦æ±‚è¿”å›JSONã€‚ç¡®ä¿æ•°å­—é”®å€¼ä¸ºæ•´æ•°ï¼Œåˆ—è¡¨ä¸ºæ•°ç»„ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=LLM_TEMPERATURE
            )
            response_content = response.choices[0].message.content
          
            # å°è¯•è§£æ JSON
            llm_output = json.loads(response_content)
            break
      
        except json.JSONDecodeError:
            tqdm.write(f"âš ï¸ API è¿”å›éæ ‡å‡† JSON (ç¬¬ {attempt + 1} æ¬¡é‡è¯•)ï¼Œå°è¯•é‡æ–°è°ƒç”¨ã€‚")
            llm_output = None
            time.sleep(2 * (attempt + 1))
        except Exception as e:
            tqdm.write(f"âš ï¸ APIè°ƒç”¨å¤±è´¥ (ç¬¬ {attempt + 1} æ¬¡é‡è¯•): {e}")
            llm_output = None
            time.sleep(2 * (attempt + 1))
          
    if llm_output is None or "match_details" not in llm_output:
        # å¤±è´¥æ—¶ï¼Œè¿”å›é»˜è®¤/å¹»è§‰ç»“æœ
        tqdm.write(f"âŒ LLMäº‹å®æ¯”å¯¹å¤±è´¥ï¼Œæ ·æœ¬ {item.get('zylsh', 'N/A')} æ ‡è®°ä¸ºå¹»è§‰ã€‚")
        # å°è¯•ä»è¾“å…¥æ•°æ®ä¸­ä¼°è®¡ M (ç”¨äº FHR)
        M_fallback = len(re.findall(r'[0-9]+[ã€\.ï¼Œ]', model_summary)) or 1
      
        return {
            "error": "LLM fact comparison failed after retries or returned bad JSON.",
            "CFCR": 0.0, "FCR": 0.0, "FHR": 1.0,
            "N": 0, "M": M_fallback, "X": 0, "Y": 0,
            "match_details": []
        }
    
    # é²æ£’è®¡ç®—ï¼šä»åˆ—è¡¨æ´¾ç”Ÿæ€»æ•°ï¼Œè¦†ç›– LLM å¯èƒ½çš„é”™è¯¯
    N_facts_list = llm_output.get("N_facts_list", [])
    N = len(N_facts_list) if isinstance(N_facts_list, list) else safe_int_convert(llm_output.get("N_total", 0))
    
    match_details = llm_output.get("match_details", [])
    if isinstance(match_details, list):
        M = len(match_details)
        X = sum(1 for d in match_details if isinstance(d, dict) and d.get('type') == 'X')
        Y = sum(1 for d in match_details if isinstance(d, dict) and d.get('type') == 'Y')
    else:
        M = safe_int_convert(llm_output.get("M_total", 0))
        X = safe_int_convert(llm_output.get("X_matched", 0))
        Y = safe_int_convert(llm_output.get("Y_matched", 0))
    
    I_facts_list = llm_output.get("I_facts_list", [])
  
    # å¼ºåˆ¶æ ¡æ­£ï¼šç¡®ä¿ X + Y ä¸è¶…è¿‡ M (æ¨¡å‹æ€»äº‹å®æ•°)
    if M > 0 and X + Y > M:
        tqdm.write(f"âš ï¸ è®¡æ•°å¼‚å¸¸: X({X}) + Y({Y}) > M({M})ã€‚ Yå·²å¼ºåˆ¶æ ¡æ­£ä¸º {M - X}ã€‚ID: {item.get('zylsh', 'N/A')}")
        Y = max(0, M - X)
      
    cfcr, fcr, fhr = calculate_metrics(N, M, X, Y)
    return {
        "CFCR": cfcr,
        "FCR": fcr,
        "FHR": fhr,
        "N": N, "M": M, "X": X, "Y": Y,
        "match_details": match_details, # è¯¦ç»†åŒ¹é…ç»“æœ
        "N_facts_list": N_facts_list,
        "I_facts_list": I_facts_list,
    }

def process_dataset(file_path, output_path, stats_output_path, client):
    """åŠ è½½æ•°æ®ï¼Œæ‰¹é‡å¤„ç†å¹¶è®¡ç®—ç»Ÿè®¡ç»“æœ (é™åˆ¶å‰MAX_SAMPLESæ¡)"""
  
    # ç§‘å®¤ç»Ÿè®¡çš„ç´¯åŠ å™¨
    dept_stats = defaultdict(lambda: {"N_total": 0, "M_total": 0, "X_total": 0, "Y_total": 0, "count": 0})
  
    # æ•´ä½“ç»Ÿè®¡çš„ç´¯åŠ å™¨
    overall_stats = {"N_total": 0, "M_total": 0, "X_total": 0, "Y_total": 0, "count": 0}
  
    # é€è¡Œè¯»å–å’Œå¤„ç†æ•°æ®
    with open(file_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:
        # ä½¿ç”¨ tqdm åŒ…è£…è¿­ä»£å™¨ä»¥æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œæ€»æ•°ä¸º MAX_SAMPLES
        for i, line in enumerate(tqdm(infile, total=MAX_SAMPLES, desc="ğŸ“Š æ­£åœ¨å¤„ç†æ ·æœ¬")):
            if i >= MAX_SAMPLES:
                break # è¾¾åˆ°é™åˆ¶ï¼Œé€€å‡ºå¾ªç¯
          
            try:
                item = json.loads(line)
              
                # --- æå–å…³é”®å­—æ®µ ---
                item_id = item.get("zylsh", "N/A")
                department = item.get("ç§‘å®¤", "æœªçŸ¥ç§‘å®¤")
              
                # --- è°ƒç”¨å¤„ç†å‡½æ•° ---
                metrics_data = process_single_item(item, client)
              
                # --- æ”¶é›†å’Œä¿å­˜ç»“æœ ---
                item_result = {
                    "id": item_id,
                    "ç§‘å®¤": department,
                    "input_data": {
                        "new_summary": item.get("new_summary", ""),
                        "output": item.get("output", ""),
                        "instruction_snippet": item.get("instruction", "")[:50] + "...",
                    },
                    "metrics": metrics_data
                }
              
                # å†™å…¥ç»“æœæ–‡ä»¶
                outfile.write(json.dumps(item_result, ensure_ascii=False) + '\n')
              
                # --- ç´¯åŠ ç»Ÿè®¡ ---
                # å®‰å…¨æå– N, M, X, Yï¼Œç¡®ä¿ä¸º int
                N = safe_int_convert(metrics_data.get("N", 0))
                M = safe_int_convert(metrics_data.get("M", 0))
                X = safe_int_convert(metrics_data.get("X", 0))
                Y = safe_int_convert(metrics_data.get("Y", 0))
                
                # ä»…å¯¹æ¨¡å‹è¿”å›æœ‰æ•ˆäº‹å®æ•°çš„æ ·æœ¬è¿›è¡Œç»Ÿè®¡
                if M > 0 or N > 0:
                    # ç§‘å®¤ç»Ÿè®¡
                    dept_stats[department]["N_total"] += N
                    dept_stats[department]["M_total"] += M
                    dept_stats[department]["X_total"] += X
                    dept_stats[department]["Y_total"] += Y
                    dept_stats[department]["count"] += 1
                    # æ•´ä½“ç»Ÿè®¡
                    overall_stats["N_total"] += N
                    overall_stats["M_total"] += M
                    overall_stats["X_total"] += X
                    overall_stats["Y_total"] += Y
                    overall_stats["count"] += 1
            except json.JSONDecodeError:
                tqdm.write(f"âŒ å¿½ç•¥è¡Œ: JSONæ ¼å¼é”™è¯¯ - {line.strip()}")
            except Exception as e:
                tqdm.write(f"âŒ å¤„ç†æ ·æœ¬ {item.get('zylsh', 'N/A')} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    
    # æ–°å¢ï¼šè®¡ç®—å¹¶ä¿å­˜ç»Ÿè®¡ç»“æœåˆ°æ–‡ä»¶
    overall_final = {
        "title": "æ•´ä½“ç»Ÿè®¡",
        "N_total": overall_stats["N_total"],
        "M_total": overall_stats["M_total"],
        "X_total": overall_stats["X_total"],
        "Y_total": overall_stats["Y_total"],
        "count": overall_stats["count"],
        "CFCR": calculate_metrics(overall_stats["N_total"], overall_stats["M_total"], overall_stats["X_total"], overall_stats["Y_total"])[0],
        "FCR": calculate_metrics(overall_stats["N_total"], overall_stats["M_total"], overall_stats["X_total"], overall_stats["Y_total"])[1],
        "FHR": calculate_metrics(overall_stats["N_total"], overall_stats["M_total"], overall_stats["X_total"], overall_stats["Y_total"])[2]
    }
    
    dept_final = {
        "title": "ç§‘å®¤ç»Ÿè®¡",
        "departments": {}
    }
    for dept, stats in dept_stats.items():
        if stats["count"] > 0:
            cfcr, fcr, fhr = calculate_metrics(stats["N_total"], stats["M_total"], stats["X_total"], stats["Y_total"])
            dept_final["departments"][dept] = {
                "N_total": stats["N_total"],
                "M_total": stats["M_total"],
                "X_total": stats["X_total"],
                "Y_total": stats["Y_total"],
                "count": stats["count"],
                "CFCR": cfcr,
                "FCR": fcr,
                "FHR": fhr
            }
    
    final_stats = {
        "overall": overall_final,
        "departments": dept_final["departments"]
    }
    
    with open(stats_output_path, 'w', encoding='utf-8') as stats_file:
        json.dump(final_stats, stats_file, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ ç»Ÿè®¡ç»“æœå·²ä¿å­˜è‡³: {stats_output_path}")
    
    return overall_stats, dept_stats

def print_statistics(stats_dict, title):
    """æ ¼å¼åŒ–å¹¶æ‰“å°ç»Ÿè®¡ç»“æœ"""
    print(f"\n--- {title} ç»Ÿè®¡ç»“æœ ({stats_dict['count']}ä¸ªæ ·æœ¬) ---")
  
    N = stats_dict["N_total"]
    M = stats_dict["M_total"]
    X = stats_dict["X_total"]
    Y = stats_dict["Y_total"]
  
    cfcr, fcr, fhr = calculate_metrics(N, M, X, Y)
  
    print(f"æ€» Gold äº‹å®æ•° (N): {N}")
    print(f"æ€» Model äº‹å®æ•° (M): {M}")
    print(f"æ€» æ ¸å¿ƒåŒ¹é…æ•° (X): {X}")
    print(f"æ€» æŒ‡ä»¤åŒ¹é…æ•° (Y): {Y}")
    print("----------------------------------------")
    print(f"âœ… æ ¸å¿ƒäº‹å®è¦†ç›–ç‡ (CFCR = X/N): {cfcr:.4f}")
    print(f"âœ… äº‹å®ç¬¦åˆç‡ (FCR = (X+Y)/M): {fcr:.4f}")
    print(f"âœ… äº‹å®å¹»è§‰ç‡ (FHR = (M-X-Y)/M): {fhr:.4f}")

def print_department_stats(dept_stats):
    """æ‰“å°æŒ‰ç§‘å®¤åˆ†ç»„çš„ç»Ÿè®¡ç»“æœ"""
    print("\n--- ğŸ¥ æŒ‰ç§‘å®¤åˆ†ç»„ç»Ÿè®¡ç»“æœ ---")
  
    table_data = []
  
    for dept, stats in dept_stats.items():
        if stats["count"] == 0:
            continue
      
        N, M, X, Y = stats["N_total"], stats["M_total"], stats["X_total"], stats["Y_total"]
        cfcr, fcr, fhr = calculate_metrics(N, M, X, Y)
      
        table_data.append({
            "ç§‘å®¤": dept,
            "æ ·æœ¬æ•°": stats["count"],
            "CFCR": f"{cfcr:.4f}",
            "FCR": f"{fcr:.4f}",
            "FHR": f"{fhr:.4f}",
        })
      
    # æ ¼å¼åŒ–è¾“å‡ºä¸º Markdown è¡¨æ ¼
    if table_data:
        table_data.sort(key=lambda x: x['æ ·æœ¬æ•°'], reverse=True)
      
        header = ["ç§‘å®¤", "æ ·æœ¬æ•°", "CFCR", "FCR", "FHR"]
        print("| " + " | ".join(header) + " |")
        print("|" + "---|"*len(header))
      
        for row in table_data:
            print(f"| {row['ç§‘å®¤']} | {row['æ ·æœ¬æ•°']} | {row['CFCR']} | {row['FCR']} | {row['FHR']} |")
    else:
        print("æ— ç§‘å®¤æ•°æ®å¯ä¾›ç»Ÿè®¡ã€‚")

# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
  
    print(f"ğŸš€ æ­£åœ¨å¯åŠ¨äº‹å®æ¯”å¯¹è®¡ç®—ã€‚æ–‡ä»¶: {FILE_PATH}")
    print(f"æ¨¡å‹: {MODEL_NAME} | é™åˆ¶æ ·æœ¬æ•°: {MAX_SAMPLES}")
    print("---------------------------------------------------------")
    overall_stats, dept_stats = process_dataset(FILE_PATH, OUTPUT_FILE_PATH, STATS_OUTPUT_PATH, client)
    # 1. æ•´ä½“è®¡ç®—å€¼
    print_statistics(overall_stats, "æ€» ä½“")
  
    # 2. æŒ‰ç§‘å®¤åˆ†ç»„è®¡ç®—å€¼
    print_department_stats(dept_stats)
  
    print(f"\nğŸ‰ æ ·æœ¬å¤„ç†å®Œæ¯•ã€‚è¯¦ç»†åŒ¹é…ç»“æœå·²ä¿å­˜è‡³ {OUTPUT_FILE_PATH}")




# æ‰€æœ‰çš„æ•°æ®
